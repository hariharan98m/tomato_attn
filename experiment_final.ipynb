{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#tensorflow, keras\n",
    "import keras\n",
    "from keras.layers import merge, Conv2D, Input, Reshape, Activation\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from numpy import genfromtxt\n",
    "from keras.preprocessing.image import img_to_array as img_to_array\n",
    "from keras.preprocessing.image import load_img as load_img\n",
    "\n",
    "#essentials\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(input_shape):\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    # First Block\n",
    "    #CONV layer\n",
    "    X = Conv2D(32, (7, 7), strides = (1,1), activation='relu', padding='same')(X_input)\n",
    "    # MAXPOOL + BatchNorm\n",
    "    X = MaxPooling2D((2,2), strides = 2, padding='same')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    \n",
    "    # Second Block\n",
    "    #CONV layer\n",
    "    X = Conv2D(64, (5, 5), strides = (2,2), activation='relu', padding='same')(X)\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2,2), strides = 2, padding='same')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    \n",
    "    # Third Block\n",
    "    X = Conv2D(128, (5,5), strides = (1, 1),activation='relu', padding='same')(X)\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D(pool_size = 3, strides = 2, padding='same')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    \n",
    "    # Top layer\n",
    "    X = AveragePooling2D(pool_size=(2,2), strides=(2,2))(X)\n",
    "    X = Conv2D(64, (7,7), strides = (2,2),activation='relu')(X)\n",
    "    \n",
    "    # L2 normalization\n",
    "    X = Lambda(lambda  x: K.l2_normalize(x,axis=-1))(X)\n",
    "\n",
    "    #Final output layer. First Unit is a sigmoid act(whether seen img is infected/not)\n",
    "    # next 2 units for identifying type of infection if 1st element is 1. otherwise, don't care.\n",
    "    \n",
    "    infected = Conv2D(2, (1, 1), strides = (1,1),activation='softmax')(X)\n",
    "    infection_type = Conv2D(3, (1, 1), strides = (1,1),activation='softmax')(X)\n",
    "    \n",
    "    infected= Reshape((2,))(infected)\n",
    "    infection_type= Reshape((3,))(infection_type)\n",
    "    # Create model instance\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = (infected, infection_type))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, size=256):\n",
    "    # data augmentation logic such as random rotations can be added here\n",
    "    return img_to_array(load_img(image_path, target_size=(size, size, 3))) / 255.\n",
    "\n",
    "class InfectedLeavesSequence(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, X, y2, batch_size=512, dim=(256,256), n_channels=3, shuffle=True):\n",
    "        #Initialization\n",
    "        self.dim = dim\n",
    "        self.X= X\n",
    "        self.y2= y2\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        batched_image_names = [self.X[k] for k in indexes]\n",
    "        batched_y2 = self.y2[indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X = self.__data_generation(batched_image_names)\n",
    "        \n",
    "        return X, batched_y2\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, images):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(images):\n",
    "            # Store sample\n",
    "            X[i,:, :, :] = load_image('data_images/Archive/'+ID)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = ''\n",
    "\n",
    "random.seed(0)\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df_path='infectedLeaves.csv', n_classes1= 2, n_classes2= 3):\n",
    "    df = pd.read_csv(df_path)\n",
    "    #resolve priyanka's error.\n",
    "    df['infectionType']= df['infectionType'].map({0: 0, 1:1, 2:2, 4:3})\n",
    "    #create categorical quants\n",
    "    y2= keras.utils.to_categorical(df['infectionType'], num_classes=n_classes2+1)\n",
    "    #drop final axis=-1 last dim of y2\n",
    "    #final images list names\n",
    "    X = df['pathName'].values\n",
    "    return df, X, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X, y2= prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of uninfected leaves 63003\n",
      "total number of infected leaves 56997\n",
      "total number of leaves with Disease1 25738\n",
      "total number of leaves with Disease1 22000\n",
      "total number of leaves with Disease1 9259\n"
     ]
    }
   ],
   "source": [
    "#data distribution\n",
    "print ('total number of uninfected leaves', sum(df['infected']==0))\n",
    "print ('total number of infected leaves', sum(df['infected']==1))\n",
    "print ('total number of leaves with Disease1', sum(df['infectionType']==0))\n",
    "print ('total number of leaves with Disease1', sum(df['infectionType']==1))\n",
    "print ('total number of leaves with Disease1', sum(df['infectionType']==2))\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred, N = 128, beta=128, epsilon=1e-8):\n",
    "    \n",
    "    infection_type = y_pred\n",
    "    \n",
    "    # loss for either layers\n",
    "    return tf.losses.softmax_cross_entropy(y_true, infection_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "cvscores = []\n",
    "\n",
    "##Setting up the path for saving logs\n",
    "logs_path = job_dir + 'logs/tensorboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9259"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y2[:, 2]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold split: Fold 1\n",
      "Training dataset size 95999\n",
      "Val dataset size 24001\n",
      "\n",
      "Train: Un-infected samples 50402\n",
      "Train: Disease 1 samples 20590\n",
      "Train: Disease 2 samples 17600\n",
      "Train: Disease 3 samples 7407\n",
      "\n",
      "Train: Un-infected samples 12601\n",
      "Train: Disease 1 samples 5148\n",
      "Train: Disease 2 samples 4400\n",
      "Train: Disease 3 samples 1852\n",
      "\n",
      "\n",
      "KFold split: Fold 2\n",
      "Training dataset size 95999\n",
      "Val dataset size 24001\n",
      "\n",
      "Train: Un-infected samples 50402\n",
      "Train: Disease 1 samples 20590\n",
      "Train: Disease 2 samples 17600\n",
      "Train: Disease 3 samples 7407\n",
      "\n",
      "Train: Un-infected samples 12601\n",
      "Train: Disease 1 samples 5148\n",
      "Train: Disease 2 samples 4400\n",
      "Train: Disease 3 samples 1852\n",
      "\n",
      "\n",
      "KFold split: Fold 3\n",
      "Training dataset size 95999\n",
      "Val dataset size 24001\n",
      "\n",
      "Train: Un-infected samples 50402\n",
      "Train: Disease 1 samples 20590\n",
      "Train: Disease 2 samples 17600\n",
      "Train: Disease 3 samples 7407\n",
      "\n",
      "Train: Un-infected samples 12601\n",
      "Train: Disease 1 samples 5148\n",
      "Train: Disease 2 samples 4400\n",
      "Train: Disease 3 samples 1852\n",
      "\n",
      "\n",
      "KFold split: Fold 4\n",
      "Training dataset size 96001\n",
      "Val dataset size 23999\n",
      "\n",
      "Train: Un-infected samples 50403\n",
      "Train: Disease 1 samples 20591\n",
      "Train: Disease 2 samples 17600\n",
      "Train: Disease 3 samples 7407\n",
      "\n",
      "Train: Un-infected samples 12600\n",
      "Train: Disease 1 samples 5147\n",
      "Train: Disease 2 samples 4400\n",
      "Train: Disease 3 samples 1852\n",
      "\n",
      "\n",
      "KFold split: Fold 5\n",
      "Training dataset size 96002\n",
      "Val dataset size 23998\n",
      "\n",
      "Train: Un-infected samples 50403\n",
      "Train: Disease 1 samples 20591\n",
      "Train: Disease 2 samples 17600\n",
      "Train: Disease 3 samples 7408\n",
      "\n",
      "Train: Un-infected samples 12600\n",
      "Train: Disease 1 samples 5147\n",
      "Train: Disease 2 samples 4400\n",
      "Train: Disease 3 samples 1851\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (train_indices, val_indices) in enumerate(kfold.split(X, df['infectionType'])):\n",
    "    print ('KFold split: Fold', (index+1))\n",
    "    print ('Training dataset size', len(train_indices))\n",
    "    print ('Val dataset size', len(val_indices))\n",
    "    y2_train, y2_val = y2[train_indices], y2[val_indices]\n",
    "    print('')\n",
    "    print ('Train: Un-infected samples', np.sum(y2_train[:, 3]==1))\n",
    "    print ('Train: Disease 1 samples', np.sum(y2_train[:, 0]==1))\n",
    "    print ('Train: Disease 2 samples', np.sum(y2_train[:, 1]==1))\n",
    "    print ('Train: Disease 3 samples', np.sum(y2_train[:, 2]==1))\n",
    "    print('')\n",
    "    print ('Train: Un-infected samples', np.sum(y2_val[:, 3]==1))\n",
    "    print ('Train: Disease 1 samples', np.sum(y2_val[:, 0]==1))\n",
    "    print ('Train: Disease 2 samples', np.sum(y2_val[:, 1]==1))\n",
    "    print ('Train: Disease 3 samples', np.sum(y2_val[:, 2]==1))\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 256, 256, 32) 4736        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 32) 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 32) 128         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 64)   51264       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 128)  204928      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 128)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 1, 64)     401472      average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 1, 64)     0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 1, 2)      130         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 1, 1, 3)      195         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 2)            0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 3)            0           conv2d_12[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 663,621\n",
      "Trainable params: 663,173\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 256, 256, 32)      4736      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 128)       204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 1, 1, 64)          401472    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 4)           260       \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 663,556\n",
      "Trainable params: 663,108\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Initializing the model\n",
    "model = ConvNet((256, 256, 3))\n",
    "\n",
    "#load weights\n",
    "model.load_weights('checkpointSave.hdf5')\n",
    "\n",
    "## Compling the model\n",
    "model.compile(optimizer = \"adam\" , loss = loss, metrics = [\"accuracy\"]);\n",
    "\n",
    "penultimate= model.layers[-5].output\n",
    "typeOfInfection = Conv2D(4, (1, 1), strides = (1,1),activation='softmax')(penultimate)\n",
    "reshapeOut= Reshape((4,))(typeOfInfection)\n",
    "\n",
    "new_model = Model(inputs= model.input, outputs= reshapeOut)\n",
    "new_model.compile(optimizer=\"adam\", loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(new_model.summary())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7ff8bc3dfa20>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7ff8bc3dfd68>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7ff8bc3dfbe0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7ff8bc3dfd30>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7ff8bc3dfcf8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7ff8bc403b70>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7ff8bc3bae80>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7ff8bc3ba438>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7ff8bc16cf98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7ff8bc128c88>\n",
      "<keras.layers.pooling.AveragePooling2D object at 0x7ff8bc1283c8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7ff8bc0c1a20>\n",
      "<keras.layers.core.Lambda object at 0x7ff8a40615c0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7ff8906ce828>\n",
      "<keras.layers.core.Reshape object at 0x7ff8906f4978>\n"
     ]
    }
   ],
   "source": [
    "for layer in new_model.layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_3:0\", shape=(32,), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1092\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3568\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder_3:0\", shape=(32,), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d9434b08fccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpointSave.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m## Compling the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                              ' elements.')\n\u001b[1;32m   1057\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2468\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1095\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_3:0\", shape=(32,), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "##Using the GPU\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "    ## Initializing the model\n",
    "    model = ConvNet((256, 256, 3))\n",
    "\n",
    "    #load weights\n",
    "    model.load_weights('checkpointSave.hdf5')\n",
    "\n",
    "    ## Compling the model\n",
    "    model.compile(optimizer = \"adam\" , loss = loss, metrics = [\"accuracy\"]);\n",
    "\n",
    "    penultimate= model.layers[-5].output\n",
    "    typeOfInfection = Conv2D(4, (1, 1), strides = (1,1),activation='softmax')(penultimate)\n",
    "    reshapeOut= Reshape((4,))(typeOfInfection)\n",
    "\n",
    "    new_model = Model(inputs= model.input, outputs= reshapeOut)\n",
    "    new_model.compile(optimizer=\"adam\", loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(new_model.summary())    \n",
    "\n",
    "    filepath=\"softmaxChangeCheckpoint.hdf5\"\n",
    "    checkpoint= ModelCheckpoint(filepath, verbose=1, save_best_only=False, save_weights_only=True, mode='auto', period=1)\n",
    "    callbacks_list = [checkpoint]    \n",
    "\n",
    "    # Loop through the indices the split() method returns\n",
    "    for index, (train_indices, val_indices) in enumerate(kfold.split(X, df['infectionType'])):\n",
    "\n",
    "        print(\"Training on fold \" + str(index+1) + \"/5...\")\n",
    "        #print (\"Evaluating on fold\" + str(index+1) +\"/5\")\n",
    "        m_train= len(train_indices)\n",
    "        m_val= len(val_indices)\n",
    "        \n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y2_train, y2_val = y2[train_indices], y2[val_indices]\n",
    "        \n",
    "        train_seq = InfectedLeavesSequence(X_train, y2_train)\n",
    "        val_seq = InfectedLeavesSequence(X_val, y2_val)\n",
    "\n",
    "       # print(model.evaluate_generator(val_seq)\n",
    "        X_train_1 = X_train[:100]\n",
    "        y_train_1 = y2_train[:100]\n",
    "        \n",
    "        X = np.empty((100, 256,256,3))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(X_train_1):\n",
    "            # Store sample\n",
    "            X[i,:, :, :] = load_image('data_images/Archive/'+ID)\n",
    "\n",
    "            \n",
    "        new_model.fit(X, y_train_1)\n",
    "        \n",
    "        #save model\n",
    "        print('exception thrown')\n",
    "        new_model.save('SoftmaxModel_fold'+str(index+1)+'.h5')            \n",
    "\n",
    "        #save model\n",
    "        new_model.save('SoftMaxModel_fold'+str(index+1)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
